{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UJIIndoorLoc with WF-DCGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-k_9m3ymT7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import scale\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# get dataset from UCI repository\n",
        "import requests\n",
        "import io\n",
        "from zipfile import ZipFile\n",
        "# UNCOMMENT\n",
        "response = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00310/UJIndoorLoc.zip')\n",
        "compressedFile = io.BytesIO(response.content)\n",
        "zipFile = ZipFile(compressedFile)\n",
        "\n",
        "# # TEMP\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XfsIqmRoX-Q",
        "colab_type": "text"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5im3xqRsmveK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATASET\n",
        "\n",
        "# UNCOMMENT\n",
        "dataset = pd.read_csv(zipFile.open('UJIndoorLoc/trainingData.csv'), header=0)\n",
        "# # TEMP\n",
        "# dataset = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/measurements/UJIIndoorLoc Data Set/trainingData.csv')\n",
        "dataset = dataset.sample(frac=1.0,random_state=0) # CHANGE TO VARY THE DATASET PERCENTAGE #@param\n",
        "\n",
        "\n",
        "features = np.asarray(dataset.iloc[:,0:520])\n",
        "# replace NaN with -110 dBm\n",
        "features[features == 100] = -110\n",
        "# feature normalization\n",
        "features = (features - features.mean()) / features.var()\n",
        "\n",
        "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
        "# convert categorical variable into dummy/indicator variables\n",
        "labels = np.asarray(pd.get_dummies(labels))\n",
        "\n",
        "# training and validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(features, labels, test_size=0.3, random_state = 0, stratify=labels)\n",
        "\n",
        "# test set\n",
        "\n",
        "# UNCOMMENT\n",
        "test_dataset = pd.read_csv(zipFile.open('UJIndoorLoc/validationData.csv'), header=0)\n",
        "# # TEMP\n",
        "# test_dataset = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/measurements/UJIIndoorLoc Data Set/validationData.csv')\n",
        "\n",
        "\n",
        "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
        "test_features[test_features == 100] = -110\n",
        "test_features = (test_features - test_features.mean()) / test_features.var()\n",
        "\n",
        "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
        "test_labels = np.asarray(pd.get_dummies(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvdNP2TK1iy8",
        "colab_type": "text"
      },
      "source": [
        "Dataset with DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dc7oCe9Izc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQE5ZaLauTPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fingerprintsToImages(class_features):\n",
        "\n",
        "  # convert fingerprints into images \n",
        "\n",
        "  height = 24 \n",
        "  width = 24\n",
        "  channels = 1 \n",
        "\n",
        "  class_images = np.zeros((class_features.shape[0],width*height))\n",
        "  class_images[:class_features.shape[0],:class_features.shape[1]] = class_features\n",
        "  class_images = class_images.reshape(class_features.shape[0],width,height)\n",
        "  class_images = np.where(class_images==0, -110, class_images)\n",
        "  class_images = class_images.reshape(class_images.shape[0], width, height, channels).astype('float32')\n",
        "\n",
        "  return class_images\n",
        "\n",
        "# Generator model\n",
        "\n",
        "def make_generator_model():\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Dense(6*6*256, use_bias=False, input_shape=(100,)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "\n",
        "  model.add(layers.Reshape((6, 6, 256)))\n",
        "  assert model.output_shape == (None, 6, 6, 256)\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 6, 6, 128)\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 12, 12, 64)\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  assert model.output_shape == (None, 24, 24, 1)\n",
        "\n",
        "  return model\n",
        "\n",
        "# Discriminator model\n",
        "\n",
        "def make_discriminator_model():\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                  input_shape=[24, 24, 1]))\n",
        "  model.add(layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "  model.add(layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(1))\n",
        "\n",
        "  return model\n",
        "\n",
        "# Training\n",
        "\n",
        "def train(dataset, epochs):\n",
        "\n",
        "  noise_dim = 100\n",
        "\n",
        "  generator = make_generator_model()\n",
        "\n",
        "  discriminator = make_discriminator_model()\n",
        "\n",
        "  # Loss and optimizer\n",
        "\n",
        "  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "  def discriminator_loss(real_output, fake_output):\n",
        "      real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "      fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "      total_loss = real_loss + fake_loss\n",
        "      return total_loss\n",
        "\n",
        "  discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "  def generator_loss(fake_output):\n",
        "      return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "  generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "  print('STARTING TRAINING')  \n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    for image_batch in dataset:\n",
        "\n",
        "      noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(image_batch, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "  print('MODEL TRAINED')  \n",
        "\n",
        "  return generator\n",
        "\n",
        "def generateSamples(generator, num_examples_to_generate):\n",
        "\n",
        "  noise_dim = 100\n",
        "\n",
        "  seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "  generated_images = generator(seed, training=False)\n",
        "\n",
        "  generated_values = generated_images.numpy().reshape(generated_images.numpy().shape[0],-1)[:,:520]\n",
        "\n",
        "  return generated_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E19vlZNm8X-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "1b8662df-289e-4a7a-e2de-2fc962152f36"
      },
      "source": [
        "labels_unique = np.unique(labels,axis=0)\n",
        "features_labels = pd.concat([pd.DataFrame(features),pd.DataFrame(labels)],axis=1)\n",
        "generated_samples_dict = {}\n",
        "\n",
        "for i in range(labels_unique.shape[0]):\n",
        "\n",
        "  print('STARTING CLASS',i)\n",
        "\n",
        "  class_i_label = labels_unique[i]\n",
        "  class_i_features = features[(features_labels.iloc[:,520:].values == class_i_label).all(axis=1)]\n",
        "  class_i_labels = labels[(features_labels.iloc[:,520:].values == class_i_label).all(axis=1)]\n",
        "\n",
        "  num_examples_to_generate = int(class_i_features.shape[0]*15.0) # CHANGE TO VARY THE DCGAN PERCENTAGE #@param\n",
        "  EPOCHS = 50 \n",
        "  BATCH_SIZE = 16\n",
        "  BUFFER_SIZE = class_i_features.shape[0]\n",
        "\n",
        "  class_i_images = fingerprintsToImages(class_i_features)\n",
        "\n",
        "  train_batch_class_i = tf.data.Dataset.from_tensor_slices(class_i_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "  generator_class_i = train(train_batch_class_i, EPOCHS)\n",
        "  generated_samples_class_i = generateSamples(generator_class_i, num_examples_to_generate)\n",
        "\n",
        "  generated_samples_dict[i] = generated_samples_class_i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STARTING CLASS 0\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 1\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 2\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 3\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 4\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 5\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 6\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 7\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 8\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 9\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 10\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 11\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n",
            "STARTING CLASS 12\n",
            "STARTING TRAINING\n",
            "MODEL TRAINED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uieg6h1oS2xB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_samples = []\n",
        "for i in range(labels_unique.shape[0]):\n",
        "  for j in range(generated_samples_dict[i].shape[0]):\n",
        "    generated_samples.append(pd.Series(np.concatenate((np.asarray(generated_samples_dict[i][j]),labels_unique[i]))))\n",
        "generated_samples = pd.DataFrame(generated_samples)\n",
        "\n",
        "# split generated samples in generated features and labels\n",
        "\n",
        "generated_features = generated_samples.iloc[:,:520]\n",
        "generated_labels = generated_samples.iloc[:,520:]\n",
        "generated_labels.columns = range(13)\n",
        "\n",
        "# extended features and labels\n",
        "\n",
        "features = pd.DataFrame(features).append(generated_features).reset_index().drop('index',axis=1).values\n",
        "labels = pd.DataFrame(labels).append(generated_labels,ignore_index=True).values\n",
        "\n",
        "# training and validation set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(features, labels, test_size=0.3, random_state = 0, stratify=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucToqY52pi93",
        "colab_type": "text"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTDJ_zqJhMmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4cfd0021-b4a5-4cd6-f125-5bc5dbd67b8b"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7h_KG4Qm3ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "9e80d9cf-762b-42c4-aac5-c29bcd473000"
      },
      "source": [
        "# NEURAL NETWORK\n",
        "\n",
        "# parameters\n",
        "\n",
        "n_input = 520 \n",
        "print(\"n_input:\",n_input)\n",
        "n_hidden_1 = 256 \n",
        "print(\"n_hidden_1:\",n_hidden_1)\n",
        "n_hidden_2 = 128 \n",
        "print(\"n_hidden_2:\",n_hidden_2)\n",
        "n_hidden_3 = 64 \n",
        "print(\"n_hidden_3:\",n_hidden_3)\n",
        "\n",
        "n_classes = labels.shape[1]\n",
        "print(\"n_classes:\",n_classes)\n",
        "\n",
        "learning_rate = 0.00001 \n",
        "print(\"learning_rate:\",learning_rate)\n",
        "training_epochs = 30 \n",
        "print(\"training_epochs:\",training_epochs)\n",
        "batch_size = 15 \n",
        "print(\"batch_size:\",batch_size)\n",
        "\n",
        "total_batches = train_x.shape[0] // batch_size\n",
        "print(\"total_batches = train_x.shape[0] // batch_size: \", train_x.shape[0], '/', batch_size, '=', total_batches)\n",
        "\n",
        "# network architecture\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.random.truncated_normal(shape, stddev = 0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.0, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
        "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
        "\n",
        "# --------------------- Encoder Variables --------------- #\n",
        "\n",
        "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
        "e_biases_h1 = bias_variable([n_hidden_1])\n",
        "\n",
        "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
        "e_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
        "e_biases_h3 = bias_variable([n_hidden_3])\n",
        "\n",
        "# --------------------- Decoder Variables --------------- #\n",
        "\n",
        "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "d_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
        "d_biases_h2 = bias_variable([n_hidden_1])\n",
        "\n",
        "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
        "d_biases_h3 = bias_variable([n_input])\n",
        "\n",
        "# --------------------- DNN Variables ------------------ #\n",
        "\n",
        "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
        "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
        "dnn_biases_out = bias_variable([n_classes])\n",
        "\n",
        "def encode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
        "    return l3\n",
        "    \n",
        "def decode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
        "    return l3\n",
        "\n",
        "def dnn(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
        "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
        "    return out\n",
        "\n",
        "encoded = encode(X)\n",
        "decoded = decode(encoded) \n",
        "y_ = dnn(encoded)\n",
        "\n",
        "# unsupervised cost function\n",
        "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
        "# supervised cost function\n",
        "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
        "\n",
        "us_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(us_cost_function)\n",
        "s_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(s_cost_function)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_input: 520\n",
            "n_hidden_1: 256\n",
            "n_hidden_2: 128\n",
            "n_hidden_3: 64\n",
            "n_classes: 13\n",
            "learning_rate: 1e-05\n",
            "training_epochs: 30\n",
            "batch_size: 15\n",
            "total_batches = train_x.shape[0] // batch_size:  223294 / 15 = 14886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFMqZXBdqG8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fc2af0c-15cb-424d-e95f-696fa6c4aad6"
      },
      "source": [
        "# RESULTS\n",
        "\n",
        "with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    \n",
        "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
        "    print(\"Unsupervised pre-training finished...\")\n",
        "    \n",
        "    \n",
        "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            batch_y = train_y[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
        "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
        "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
        "            \n",
        "    print(\"Supervised training finished...\")\n",
        "    \n",
        "\n",
        "    print(\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  0.008527898416315315\n",
            "Epoch:  1  Loss:  0.0013310450161482064\n",
            "Epoch:  2  Loss:  0.0010579160595574136\n",
            "Epoch:  3  Loss:  0.0009355233620564001\n",
            "Epoch:  4  Loss:  0.0008624084733557282\n",
            "Epoch:  5  Loss:  0.0008137759219937858\n",
            "Epoch:  6  Loss:  0.0007775002852549744\n",
            "Epoch:  7  Loss:  0.0007482773153228864\n",
            "Epoch:  8  Loss:  0.0007237951807255926\n",
            "Epoch:  9  Loss:  0.0007030208613537703\n",
            "Epoch:  10  Loss:  0.000685388531403182\n",
            "Epoch:  11  Loss:  0.0006704666631321193\n",
            "Epoch:  12  Loss:  0.0006578151984096639\n",
            "Epoch:  13  Loss:  0.0006469721233822475\n",
            "Epoch:  14  Loss:  0.0006375386200691929\n",
            "Epoch:  15  Loss:  0.0006292323131490346\n",
            "Epoch:  16  Loss:  0.0006218679271268\n",
            "Epoch:  17  Loss:  0.0006153138946460533\n",
            "Epoch:  18  Loss:  0.0006094622878076091\n",
            "Epoch:  19  Loss:  0.0006042194837503487\n",
            "Epoch:  20  Loss:  0.0005995081895561872\n",
            "Epoch:  21  Loss:  0.0005952695734819727\n",
            "Epoch:  22  Loss:  0.0005914601656674382\n",
            "Epoch:  23  Loss:  0.0005880448579963058\n",
            "Epoch:  24  Loss:  0.0005849904190185481\n",
            "Epoch:  25  Loss:  0.0005822625162234337\n",
            "Epoch:  26  Loss:  0.0005798259380456802\n",
            "Epoch:  27  Loss:  0.000577646196086455\n",
            "Epoch:  28  Loss:  0.0005756910105563818\n",
            "Epoch:  29  Loss:  0.0005739312279269773\n",
            "Unsupervised pre-training finished...\n",
            "Epoch:  0  Loss:  2.996595962056531  Training Accuracy:  0.9978683 Validation Accuracy: 0.9979101\n",
            "Epoch:  1  Loss:  0.13274898305471503  Training Accuracy:  0.999167 Validation Accuracy: 0.9991849\n",
            "Epoch:  2  Loss:  0.06014024949912777  Training Accuracy:  0.9993775 Validation Accuracy: 0.99933124\n",
            "Epoch:  3  Loss:  0.03952366487439894  Training Accuracy:  0.9995432 Validation Accuracy: 0.99944615\n",
            "Epoch:  4  Loss:  0.029953163565994177  Training Accuracy:  0.9996328 Validation Accuracy: 0.99945664\n",
            "Epoch:  5  Loss:  0.02437620104207523  Training Accuracy:  0.9996686 Validation Accuracy: 0.999488\n",
            "Epoch:  6  Loss:  0.020737998340916764  Training Accuracy:  0.9997089 Validation Accuracy: 0.99955064\n",
            "Epoch:  7  Loss:  0.01819679137911155  Training Accuracy:  0.99972683 Validation Accuracy: 0.99956113\n",
            "Epoch:  8  Loss:  0.016330365940942098  Training Accuracy:  0.9997537 Validation Accuracy: 0.99955064\n",
            "Epoch:  9  Loss:  0.014901576059967067  Training Accuracy:  0.9997582 Validation Accuracy: 0.99955064\n",
            "Epoch:  10  Loss:  0.013766861205479652  Training Accuracy:  0.9997716 Validation Accuracy: 0.9995402\n",
            "Epoch:  11  Loss:  0.01283655741081834  Training Accuracy:  0.99978507 Validation Accuracy: 0.99955064\n",
            "Epoch:  12  Loss:  0.012053711463417544  Training Accuracy:  0.99978507 Validation Accuracy: 0.99957156\n",
            "Epoch:  13  Loss:  0.011381356163498118  Training Accuracy:  0.99978954 Validation Accuracy: 0.99956113\n",
            "Epoch:  14  Loss:  0.010793862106371706  Training Accuracy:  0.99980295 Validation Accuracy: 0.99956113\n",
            "Epoch:  15  Loss:  0.010273168702725136  Training Accuracy:  0.9998074 Validation Accuracy: 0.99956113\n",
            "Epoch:  16  Loss:  0.009807815192976715  Training Accuracy:  0.99981636 Validation Accuracy: 0.99957156\n",
            "Epoch:  17  Loss:  0.009390608019676278  Training Accuracy:  0.9998209 Validation Accuracy: 0.999582\n",
            "Epoch:  18  Loss:  0.009017855953705092  Training Accuracy:  0.9998298 Validation Accuracy: 0.99957156\n",
            "Epoch:  19  Loss:  0.008685489096697127  Training Accuracy:  0.9998298 Validation Accuracy: 0.99957156\n",
            "Epoch:  20  Loss:  0.008387965914567634  Training Accuracy:  0.9998343 Validation Accuracy: 0.99957156\n",
            "Epoch:  21  Loss:  0.008119927413241714  Training Accuracy:  0.9998343 Validation Accuracy: 0.999582\n",
            "Epoch:  22  Loss:  0.007876544183668397  Training Accuracy:  0.9998298 Validation Accuracy: 0.999582\n",
            "Epoch:  23  Loss:  0.007654927992342712  Training Accuracy:  0.9998298 Validation Accuracy: 0.999582\n",
            "Epoch:  24  Loss:  0.00745219398552295  Training Accuracy:  0.9998298 Validation Accuracy: 0.9995925\n",
            "Epoch:  25  Loss:  0.007266926269985106  Training Accuracy:  0.9998343 Validation Accuracy: 0.9995925\n",
            "Epoch:  26  Loss:  0.007097328042109021  Training Accuracy:  0.9998343 Validation Accuracy: 0.9995925\n",
            "Epoch:  27  Loss:  0.006942383348169011  Training Accuracy:  0.9998343 Validation Accuracy: 0.9995925\n",
            "Epoch:  28  Loss:  0.0068014821357254346  Training Accuracy:  0.9998343 Validation Accuracy: 0.9995925\n",
            "Epoch:  29  Loss:  0.006673672853486703  Training Accuracy:  0.9998343 Validation Accuracy: 0.99961334\n",
            "Supervised training finished...\n",
            "\n",
            "Testing Accuracy: 0.9027903\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}