{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UJIIndoorLoc with WF-LERP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "slJTAyS1GyYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# get dataset from UCI repository\n",
        "import requests\n",
        "import io\n",
        "from zipfile import ZipFile\n",
        "response = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00310/UJIndoorLoc.zip')\n",
        "compressedFile = io.BytesIO(response.content)\n",
        "zipFile = ZipFile(compressedFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYxQ_g1KHDlA",
        "colab_type": "text"
      },
      "source": [
        "Initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4kISlliHGgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAINING AND VALIDATION SET\n",
        "\n",
        "dataset = pd.read_csv(zipFile.open('UJIndoorLoc/trainingData.csv'), header=0)\n",
        "dataset = dataset.sample(frac=1.00,random_state=0) # CHANGE TO VARY THE DATASET PERCENTAGE #@param\n",
        "\n",
        "features = np.asarray(dataset.iloc[:,0:520])\n",
        "# replace NaN with -110 dBm\n",
        "features[features == 100] = -110\n",
        "# feature normalization\n",
        "features = (features - features.mean()) / features.var()\n",
        "\n",
        "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
        "# convert categorical variable into dummy/indicator variables\n",
        "labels = np.asarray(pd.get_dummies(labels))\n",
        "\n",
        "# TEST SET\n",
        "\n",
        "test_dataset = pd.read_csv(zipFile.open('UJIndoorLoc/validationData.csv'), header=0)\n",
        "\n",
        "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
        "test_features[test_features == 100] = -110\n",
        "test_features = (test_features - test_features.mean()) / test_features.var()\n",
        "\n",
        "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
        "test_labels = np.asarray(pd.get_dummies(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IR1CsVcHgTc",
        "colab_type": "text"
      },
      "source": [
        "Augmented dataset with INTERPOLATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StNkQHiGHn1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_unique = np.unique(labels,axis=0)\n",
        "features_labels = pd.concat([pd.DataFrame(features),pd.DataFrame(labels)],axis=1)\n",
        "generated_samples_dict = {}\n",
        "\n",
        "for i in range(labels_unique.shape[0]):\n",
        "\n",
        "  class_i_label = labels_unique[i]\n",
        "  class_i_features = features[(features_labels.iloc[:,520:].values == class_i_label).all(axis=1)]\n",
        "  class_i_labels = labels[(features_labels.iloc[:,520:].values == class_i_label).all(axis=1)]\n",
        "\n",
        "  x = np.arange(0, class_i_features.shape[0], 1)\n",
        "  y = np.arange(0, 520, 1)\n",
        "  z = class_i_features\n",
        "\n",
        "  from scipy.interpolate import interp2d\n",
        "  z = np.array(z).T\n",
        "  f = interp2d(x, y, z) \n",
        "\n",
        "  num_examples_to_generate = int(class_i_features.shape[0]*15.0) # CHANGE TO VARY THE INTERPOL PERCENTAGE #@param\n",
        "\n",
        "  generated_indexes = np.random.uniform(0,class_i_features.shape[0]-1,num_examples_to_generate)\n",
        "  generated_samples_class_i = []\n",
        "  for generated_index in generated_indexes:\n",
        "    generated_samples_class_i.append(f(x=generated_index,y=np.arange(0, 520, 1)).reshape((1,-1)).flatten())\n",
        "  generated_samples_dict[i] = np.asarray(generated_samples_class_i)\n",
        "\n",
        "generated_samples = []\n",
        "for i in range(labels_unique.shape[0]):\n",
        "  for j in range(generated_samples_dict[i].shape[0]):\n",
        "    generated_samples.append(pd.Series(np.concatenate((np.asarray(generated_samples_dict[i][j]),labels_unique[i]))))\n",
        "generated_samples = pd.DataFrame(generated_samples)\n",
        "\n",
        "# split generated samples in generated features and labels\n",
        "\n",
        "generated_features = generated_samples.iloc[:,:520]\n",
        "generated_labels = generated_samples.iloc[:,520:]\n",
        "generated_labels.columns = range(13)\n",
        "\n",
        "# extended features and labels\n",
        "\n",
        "features = pd.DataFrame(features).append(generated_features).reset_index().drop('index',axis=1).values\n",
        "labels = pd.DataFrame(labels).append(generated_labels,ignore_index=True).values\n",
        "\n",
        "# training and validation set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(features, labels, test_size=0.3, random_state = 0, stratify=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo-lfOwPc7gu",
        "colab_type": "text"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2Grdzqc9Eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "0dfc4d01-8ec8-48eb-950b-8935e2f40686"
      },
      "source": [
        "# NEURAL NETWORK\n",
        "\n",
        "# parameters\n",
        "\n",
        "n_input = 520 \n",
        "print(\"n_input:\",n_input)\n",
        "n_hidden_1 = 256 \n",
        "print(\"n_hidden_1:\",n_hidden_1)\n",
        "n_hidden_2 = 128 \n",
        "print(\"n_hidden_2:\",n_hidden_2)\n",
        "n_hidden_3 = 64 \n",
        "print(\"n_hidden_3:\",n_hidden_3)\n",
        "\n",
        "n_classes = labels.shape[1]\n",
        "print(\"n_classes:\",n_classes)\n",
        "\n",
        "learning_rate = 0.00001 \n",
        "print(\"learning_rate:\",learning_rate)\n",
        "training_epochs = 30 \n",
        "print(\"training_epochs:\",training_epochs)\n",
        "batch_size = 15 \n",
        "print(\"batch_size:\",batch_size)\n",
        "\n",
        "total_batches = train_x.shape[0] // batch_size\n",
        "print(\"total_batches = train_x.shape[0] // batch_size: \", train_x.shape[0], '/', batch_size, '=', total_batches)\n",
        "\n",
        "# network architecture\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.random.truncated_normal(shape, stddev = 0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.0, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
        "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
        "\n",
        "# --------------------- Encoder Variables --------------- #\n",
        "\n",
        "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
        "e_biases_h1 = bias_variable([n_hidden_1])\n",
        "\n",
        "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
        "e_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
        "e_biases_h3 = bias_variable([n_hidden_3])\n",
        "\n",
        "# --------------------- Decoder Variables --------------- #\n",
        "\n",
        "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "d_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
        "d_biases_h2 = bias_variable([n_hidden_1])\n",
        "\n",
        "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
        "d_biases_h3 = bias_variable([n_input])\n",
        "\n",
        "# --------------------- DNN Variables ------------------ #\n",
        "\n",
        "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
        "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
        "dnn_biases_out = bias_variable([n_classes])\n",
        "\n",
        "def encode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
        "    return l3\n",
        "    \n",
        "def decode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
        "    return l3\n",
        "\n",
        "def dnn(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
        "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
        "    return out\n",
        "\n",
        "encoded = encode(X)\n",
        "decoded = decode(encoded) \n",
        "y_ = dnn(encoded)\n",
        "\n",
        "# unsupervised cost function\n",
        "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
        "# supervised cost function\n",
        "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
        "\n",
        "us_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(us_cost_function)\n",
        "s_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(s_cost_function)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_input: 520\n",
            "n_hidden_1: 256\n",
            "n_hidden_2: 128\n",
            "n_hidden_3: 64\n",
            "n_classes: 13\n",
            "learning_rate: 1e-05\n",
            "training_epochs: 30\n",
            "batch_size: 15\n",
            "total_batches = train_x.shape[0] // batch_size:  223294 / 15 = 14886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwevDGpwc-_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "313390c9-ee04-4445-8662-f142c1c63d07"
      },
      "source": [
        "# RESULTS\n",
        "\n",
        "with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    \n",
        "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
        "    print(\"Unsupervised pre-training finished...\")\n",
        "    \n",
        "    \n",
        "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            batch_y = train_y[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
        "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
        "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
        "            \n",
        "    print(\"Supervised training finished...\")\n",
        "    \n",
        "\n",
        "    print(\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  0.012447323708384092\n",
            "Epoch:  1  Loss:  0.004519258522106119\n",
            "Epoch:  2  Loss:  0.0036022981350633224\n",
            "Epoch:  3  Loss:  0.0031686468337457684\n",
            "Epoch:  4  Loss:  0.0029317737100373123\n",
            "Epoch:  5  Loss:  0.002804369711087889\n",
            "Epoch:  6  Loss:  0.0027264671015313434\n",
            "Epoch:  7  Loss:  0.0026725408418488694\n",
            "Epoch:  8  Loss:  0.0026328002279446664\n",
            "Epoch:  9  Loss:  0.00260270497954269\n",
            "Epoch:  10  Loss:  0.0025798622782405452\n",
            "Epoch:  11  Loss:  0.0025626202081940742\n",
            "Epoch:  12  Loss:  0.0025492137847672215\n",
            "Epoch:  13  Loss:  0.0025382085210968663\n",
            "Epoch:  14  Loss:  0.0025287626605498053\n",
            "Epoch:  15  Loss:  0.0025204368339099233\n",
            "Epoch:  16  Loss:  0.0025129652565983192\n",
            "Epoch:  17  Loss:  0.0025061489613709843\n",
            "Epoch:  18  Loss:  0.0024998250812766705\n",
            "Epoch:  19  Loss:  0.0024938497294410443\n",
            "Epoch:  20  Loss:  0.0024880923135241137\n",
            "Epoch:  21  Loss:  0.002482445434792047\n",
            "Epoch:  22  Loss:  0.0024768427843569582\n",
            "Epoch:  23  Loss:  0.0024712776622848077\n",
            "Epoch:  24  Loss:  0.002465809063295116\n",
            "Epoch:  25  Loss:  0.002460537334181089\n",
            "Epoch:  26  Loss:  0.002455550543759326\n",
            "Epoch:  27  Loss:  0.0024508783248281697\n",
            "Epoch:  28  Loss:  0.0024464884830377124\n",
            "Epoch:  29  Loss:  0.00244231693467814\n",
            "Unsupervised pre-training finished...\n",
            "Epoch:  0  Loss:  5.108778875410741  Training Accuracy:  0.99745625 Validation Accuracy: 0.9973354\n",
            "Epoch:  1  Loss:  0.16072325629177028  Training Accuracy:  0.99871916 Validation Accuracy: 0.9987147\n",
            "Epoch:  2  Loss:  0.07601384283834875  Training Accuracy:  0.99912673 Validation Accuracy: 0.99910134\n",
            "Epoch:  3  Loss:  0.049703099581509445  Training Accuracy:  0.9993372 Validation Accuracy: 0.99934167\n",
            "Epoch:  4  Loss:  0.03710302272662631  Training Accuracy:  0.99947155 Validation Accuracy: 0.9994148\n",
            "Epoch:  5  Loss:  0.02980942241721762  Training Accuracy:  0.99953425 Validation Accuracy: 0.99945664\n",
            "Epoch:  6  Loss:  0.02510388059879802  Training Accuracy:  0.9995701 Validation Accuracy: 0.9995402\n",
            "Epoch:  7  Loss:  0.021852162201274266  Training Accuracy:  0.99962384 Validation Accuracy: 0.99957156\n",
            "Epoch:  8  Loss:  0.019492070850183885  Training Accuracy:  0.999682 Validation Accuracy: 0.99962384\n",
            "Epoch:  9  Loss:  0.017715160643806026  Training Accuracy:  0.999691 Validation Accuracy: 0.9996656\n",
            "Epoch:  10  Loss:  0.01633533658767185  Training Accuracy:  0.99971336 Validation Accuracy: 0.9996552\n",
            "Epoch:  11  Loss:  0.01523205026097105  Training Accuracy:  0.99971783 Validation Accuracy: 0.9996656\n",
            "Epoch:  12  Loss:  0.014324833001127173  Training Accuracy:  0.9997313 Validation Accuracy: 0.9996656\n",
            "Epoch:  13  Loss:  0.013559827975612058  Training Accuracy:  0.99974024 Validation Accuracy: 0.99967605\n",
            "Epoch:  14  Loss:  0.012903067991523113  Training Accuracy:  0.9997447 Validation Accuracy: 0.99967605\n",
            "Epoch:  15  Loss:  0.01233468939494592  Training Accuracy:  0.9997492 Validation Accuracy: 0.99968654\n",
            "Epoch:  16  Loss:  0.01184227890015548  Training Accuracy:  0.9997582 Validation Accuracy: 0.99967605\n",
            "Epoch:  17  Loss:  0.011415564671771154  Training Accuracy:  0.9997582 Validation Accuracy: 0.99967605\n",
            "Epoch:  18  Loss:  0.011044945053306911  Training Accuracy:  0.9997582 Validation Accuracy: 0.99967605\n",
            "Epoch:  19  Loss:  0.010722082927631934  Training Accuracy:  0.9997582 Validation Accuracy: 0.9996656\n",
            "Epoch:  20  Loss:  0.010440059967518638  Training Accuracy:  0.99976265 Validation Accuracy: 0.9996656\n",
            "Epoch:  21  Loss:  0.010193067217852595  Training Accuracy:  0.9997582 Validation Accuracy: 0.99969697\n",
            "Epoch:  22  Loss:  0.00997636156633119  Training Accuracy:  0.9997582 Validation Accuracy: 0.99969697\n",
            "Epoch:  23  Loss:  0.00978617560747855  Training Accuracy:  0.9997582 Validation Accuracy: 0.9997074\n",
            "Epoch:  24  Loss:  0.009619425522687597  Training Accuracy:  0.9997582 Validation Accuracy: 0.9997074\n",
            "Epoch:  25  Loss:  0.009473063861434369  Training Accuracy:  0.9997582 Validation Accuracy: 0.9997074\n",
            "Epoch:  26  Loss:  0.009343658555813043  Training Accuracy:  0.9997582 Validation Accuracy: 0.9997074\n",
            "Epoch:  27  Loss:  0.009228097611399082  Training Accuracy:  0.9997671 Validation Accuracy: 0.9997074\n",
            "Epoch:  28  Loss:  0.009124186464935291  Training Accuracy:  0.9997671 Validation Accuracy: 0.9997074\n",
            "Epoch:  29  Loss:  0.009030767630031107  Training Accuracy:  0.9997716 Validation Accuracy: 0.9997074\n",
            "Supervised training finished...\n",
            "\n",
            "Testing Accuracy: 0.90819085\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}