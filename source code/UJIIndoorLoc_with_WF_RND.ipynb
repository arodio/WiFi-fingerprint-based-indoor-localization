{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UJIIndoorLoc with WF-RND.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYbmmSt2pvkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# get dataset from UCI repository\n",
        "import requests\n",
        "import io\n",
        "from zipfile import ZipFile\n",
        "response = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00310/UJIndoorLoc.zip')\n",
        "compressedFile = io.BytesIO(response.content)\n",
        "zipFile = ZipFile(compressedFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0cO9skWp2oI",
        "colab_type": "text"
      },
      "source": [
        "Initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VF53vgXp3yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAINING AND VALIDATION SET\n",
        "\n",
        "dataset = pd.read_csv(zipFile.open('UJIndoorLoc/trainingData.csv'), header=0)\n",
        "dataset = dataset.sample(frac=0.01,random_state=0) # CHANGE TO VARY THE DATASET PERCENTAGE #@param\n",
        "\n",
        "features = np.asarray(dataset.iloc[:,0:520])\n",
        "features[features == 100] = -110\n",
        "\n",
        "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
        "\n",
        "# TEST SET\n",
        "\n",
        "test_dataset = pd.read_csv(zipFile.open('UJIndoorLoc/validationData.csv'), header=0)\n",
        "\n",
        "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
        "test_features[test_features == 100] = -110\n",
        "test_features = (test_features - test_features.mean()) / test_features.var()\n",
        "\n",
        "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
        "test_labels = np.asarray(pd.get_dummies(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuY27_6ap_3E",
        "colab_type": "text"
      },
      "source": [
        "Augmented dataset with completely random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JroIarptqCXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extracted_samples = dataset.sample(frac=30.0, replace=True, random_state=0) # CHANGE TO VARY THE GENERATED PERCENTAGE #@param\n",
        "random_noise = np.random.uniform(low=-10.0, high=10.0, size=(extracted_samples.shape[0],520))\n",
        "generated_samples = extracted_samples.iloc[:,:520].replace(100, -110).add(random_noise).reset_index(drop=True)\n",
        "generated_samples = generated_samples.clip(lower=-110, upper=0)\n",
        "generated_samples = generated_samples.join(extracted_samples.iloc[:,520:].reset_index(drop=True))\n",
        "\n",
        "# split generated samples in generated features and labels\n",
        "\n",
        "generated_features = np.asarray(generated_samples.iloc[:,:520])\n",
        "generated_labels = np.asarray(extracted_samples[\"BUILDINGID\"].map(str) + extracted_samples[\"FLOOR\"].map(str))\n",
        "\n",
        "augmented_features = np.concatenate((features,generated_features))\n",
        "augmented_features = (augmented_features - augmented_features.mean()) / augmented_features.var()\n",
        "\n",
        "augmented_labels = np.concatenate((labels,generated_labels))\n",
        "augmented_labels = np.asarray(pd.get_dummies(augmented_labels))\n",
        "\n",
        "features = augmented_features\n",
        "labels = augmented_labels\n",
        "\n",
        "# training and validation set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, val_x, train_y, val_y = train_test_split(features, labels, test_size=0.3, random_state = 0, stratify=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhfBzN7IsG2I",
        "colab_type": "text"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIU7SDX-rby7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "792978f1-d7e9-47e5-802f-b3589fa53fdf"
      },
      "source": [
        "# NEURAL NETWORK\n",
        "\n",
        "# parameters\n",
        "\n",
        "n_input = 520 \n",
        "print(\"n_input:\",n_input)\n",
        "n_hidden_1 = 256 \n",
        "print(\"n_hidden_1:\",n_hidden_1)\n",
        "n_hidden_2 = 128 \n",
        "print(\"n_hidden_2:\",n_hidden_2)\n",
        "n_hidden_3 = 64 \n",
        "print(\"n_hidden_3:\",n_hidden_3)\n",
        "\n",
        "n_classes = labels.shape[1]\n",
        "print(\"n_classes:\",n_classes)\n",
        "\n",
        "learning_rate = 0.00001 \n",
        "print(\"learning_rate:\",learning_rate)\n",
        "training_epochs = 30 \n",
        "print(\"training_epochs:\",training_epochs)\n",
        "batch_size = 15 \n",
        "print(\"batch_size:\",batch_size)\n",
        "\n",
        "total_batches = train_x.shape[0] // batch_size\n",
        "print(\"total_batches = train_x.shape[0] // batch_size: \", train_x.shape[0], '/', batch_size, '=', total_batches)\n",
        "\n",
        "# network architecture\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.random.truncated_normal(shape, stddev = 0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.0, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
        "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
        "\n",
        "# --------------------- Encoder Variables --------------- #\n",
        "\n",
        "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
        "e_biases_h1 = bias_variable([n_hidden_1])\n",
        "\n",
        "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
        "e_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
        "e_biases_h3 = bias_variable([n_hidden_3])\n",
        "\n",
        "# --------------------- Decoder Variables --------------- #\n",
        "\n",
        "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "d_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
        "d_biases_h2 = bias_variable([n_hidden_1])\n",
        "\n",
        "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
        "d_biases_h3 = bias_variable([n_input])\n",
        "\n",
        "# --------------------- DNN Variables ------------------ #\n",
        "\n",
        "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
        "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
        "dnn_biases_out = bias_variable([n_classes])\n",
        "\n",
        "def encode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
        "    return l3\n",
        "    \n",
        "def decode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
        "    return l3\n",
        "\n",
        "def dnn(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
        "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
        "    return out\n",
        "\n",
        "encoded = encode(X)\n",
        "decoded = decode(encoded) \n",
        "y_ = dnn(encoded)\n",
        "\n",
        "# unsupervised cost function\n",
        "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
        "# supervised cost function\n",
        "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
        "\n",
        "us_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(us_cost_function)\n",
        "s_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(s_cost_function)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_input: 520\n",
            "n_hidden_1: 256\n",
            "n_hidden_2: 128\n",
            "n_hidden_3: 64\n",
            "n_classes: 13\n",
            "learning_rate: 1e-05\n",
            "training_epochs: 30\n",
            "batch_size: 15\n",
            "total_batches = train_x.shape[0] // batch_size:  4318 / 15 = 287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np2g0PkzsKvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70103d5d-5ff8-4c99-8408-49a034c1b506"
      },
      "source": [
        "# RESULTS\n",
        "\n",
        "with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    \n",
        "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
        "    print(\"Unsupervised pre-training finished...\")\n",
        "    \n",
        "    \n",
        "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            batch_y = train_y[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
        "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
        "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
        "            \n",
        "    print(\"Supervised training finished...\")\n",
        "    \n",
        "\n",
        "    print(\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  0.07511457765818888\n",
            "Epoch:  1  Loss:  0.057687877180892\n",
            "Epoch:  2  Loss:  0.047971065282717815\n",
            "Epoch:  3  Loss:  0.04149525734605689\n",
            "Epoch:  4  Loss:  0.03674957199664689\n",
            "Epoch:  5  Loss:  0.033052263567256594\n",
            "Epoch:  6  Loss:  0.030045991860823348\n",
            "Epoch:  7  Loss:  0.027530731010260483\n",
            "Epoch:  8  Loss:  0.025394424692263586\n",
            "Epoch:  9  Loss:  0.0235745572397414\n",
            "Epoch:  10  Loss:  0.022028687692373886\n",
            "Epoch:  11  Loss:  0.020714918813483224\n",
            "Epoch:  12  Loss:  0.019589820866982487\n",
            "Epoch:  13  Loss:  0.018616538144362513\n",
            "Epoch:  14  Loss:  0.017768433713435297\n",
            "Epoch:  15  Loss:  0.017026004693000366\n",
            "Epoch:  16  Loss:  0.01637293883836228\n",
            "Epoch:  17  Loss:  0.015794503294866053\n",
            "Epoch:  18  Loss:  0.015277749891246861\n",
            "Epoch:  19  Loss:  0.014812107201185375\n",
            "Epoch:  20  Loss:  0.014389509082048196\n",
            "Epoch:  21  Loss:  0.01400397765979119\n",
            "Epoch:  22  Loss:  0.013650973940767893\n",
            "Epoch:  23  Loss:  0.013326827764433228\n",
            "Epoch:  24  Loss:  0.013028372893635612\n",
            "Epoch:  25  Loss:  0.01275277471994066\n",
            "Epoch:  26  Loss:  0.012497479076584873\n",
            "Epoch:  27  Loss:  0.012260212118409651\n",
            "Epoch:  28  Loss:  0.01203898638357061\n",
            "Epoch:  29  Loss:  0.011832090292632165\n",
            "Unsupervised pre-training finished...\n",
            "Epoch:  0  Loss:  36.16264753973027  Training Accuracy:  0.38953218 Validation Accuracy: 0.3851972\n",
            "Epoch:  1  Loss:  31.716206334609186  Training Accuracy:  0.5352015 Validation Accuracy: 0.54294974\n",
            "Epoch:  2  Loss:  27.434603714361423  Training Accuracy:  0.60444653 Validation Accuracy: 0.60345757\n",
            "Epoch:  3  Loss:  23.602476146578372  Training Accuracy:  0.6551644 Validation Accuracy: 0.66342515\n",
            "Epoch:  4  Loss:  20.424434203304063  Training Accuracy:  0.6968504 Validation Accuracy: 0.6980011\n",
            "Epoch:  5  Loss:  17.85437616155538  Training Accuracy:  0.73761 Validation Accuracy: 0.7379795\n",
            "Epoch:  6  Loss:  15.74665830525787  Training Accuracy:  0.7735062 Validation Accuracy: 0.77525663\n",
            "Epoch:  7  Loss:  13.963438213494596  Training Accuracy:  0.8094025 Validation Accuracy: 0.8055105\n",
            "Epoch:  8  Loss:  12.414149274394072  Training Accuracy:  0.83696157 Validation Accuracy: 0.8314425\n",
            "Epoch:  9  Loss:  11.050671185350584  Training Accuracy:  0.8589625 Validation Accuracy: 0.84602916\n",
            "Epoch:  10  Loss:  9.846572475566266  Training Accuracy:  0.87656325 Validation Accuracy: 0.86439764\n",
            "Epoch:  11  Loss:  8.782472746712822  Training Accuracy:  0.8932376 Validation Accuracy: 0.88438684\n",
            "Epoch:  12  Loss:  7.841068663248202  Training Accuracy:  0.9110699 Validation Accuracy: 0.89843327\n",
            "Epoch:  13  Loss:  7.006578192893636  Training Accuracy:  0.9231126 Validation Accuracy: 0.9092383\n",
            "Epoch:  14  Loss:  6.264952385467104  Training Accuracy:  0.93422884 Validation Accuracy: 0.9249055\n",
            "Epoch:  15  Loss:  5.604029522540262  Training Accuracy:  0.9441871 Validation Accuracy: 0.93246895\n",
            "Epoch:  16  Loss:  5.013591481418144  Training Accuracy:  0.9534507 Validation Accuracy: 0.9448947\n",
            "Epoch:  17  Loss:  4.485240888097145  Training Accuracy:  0.96155626 Validation Accuracy: 0.95407885\n",
            "Epoch:  18  Loss:  4.012090035016528  Training Accuracy:  0.9705882 Validation Accuracy: 0.9632631\n",
            "Epoch:  19  Loss:  3.5883636541067516  Training Accuracy:  0.97684115 Validation Accuracy: 0.96920586\n",
            "Epoch:  20  Loss:  3.209047313351249  Training Accuracy:  0.9823993 Validation Accuracy: 0.97514856\n",
            "Epoch:  21  Loss:  2.8696554895062065  Training Accuracy:  0.9872626 Validation Accuracy: 0.97947055\n",
            "Epoch:  22  Loss:  2.5661200139580704  Training Accuracy:  0.9895785 Validation Accuracy: 0.98379254\n",
            "Epoch:  23  Loss:  2.294755120725997  Training Accuracy:  0.99143124 Validation Accuracy: 0.9843328\n",
            "Epoch:  24  Loss:  2.052241260581731  Training Accuracy:  0.9932839 Validation Accuracy: 0.98811454\n",
            "Epoch:  25  Loss:  1.8355997040712044  Training Accuracy:  0.99536824 Validation Accuracy: 0.991356\n",
            "Epoch:  26  Loss:  1.6421551899627527  Training Accuracy:  0.99629456 Validation Accuracy: 0.99351704\n",
            "Epoch:  27  Loss:  1.4694971722592876  Training Accuracy:  0.9965262 Validation Accuracy: 0.99405724\n",
            "Epoch:  28  Loss:  1.3154449336204794  Training Accuracy:  0.99698937 Validation Accuracy: 0.99405724\n",
            "Epoch:  29  Loss:  1.1780204835253725  Training Accuracy:  0.9981473 Validation Accuracy: 0.99405724\n",
            "Supervised training finished...\n",
            "\n",
            "Testing Accuracy: 0.8361836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55uLdPW9EbO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}